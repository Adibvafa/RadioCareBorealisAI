{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import ViTFeatureExtractor, TFAutoModelForImageClassification\n",
    "from transformers import ViTForImageClassification, ViTConfig, ViTImageProcessor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DATA_DIR  = 'mimic-data/'\n",
    "ROOT = os.path.dirname(os.getcwd())\n",
    "os.chdir(f'E:/RadioCareBorealisAI')\n",
    "\n",
    "from data_modules.mimic_cxr import MimicIVCXR\n",
    "\n",
    "def seed_everything(seed: int) -> None:\n",
    "    \"\"\" Seed everything for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    seed = 23\n",
    "    train_batch_size = 48\n",
    "    valid_batch_size = 16\n",
    "    test_batch_size = 16\n",
    "    num_labels = 2\n",
    "    num_epochs = 5\n",
    "\n",
    "# Set your device\n",
    "seed_everything(args.seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_report_dir = f\"{DATA_DIR}/graph_report.csv\"\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "dataset = MimicIVCXR(data_root=DATA_DIR,\n",
    "                     graph_report_dir=graph_report_dir,\n",
    "                     tokenizer=\"AutoTokenizer.from_pretrained('bert-base-uncased')\",\n",
    "                     max_length=3000,\n",
    "                     transform=processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "[tensor([[[-0.8745, -0.7961, -0.7961,  ...,  0.6784,  0.6235,  0.5451],\n",
      "         [-0.8667, -0.7882, -0.7882,  ...,  0.7569,  0.7020,  0.6314],\n",
      "         [-0.8667, -0.7882, -0.7882,  ...,  0.7569,  0.7020,  0.6392],\n",
      "         ...,\n",
      "         [ 0.0824,  0.7176,  0.7333,  ..., -0.7882, -0.7882, -0.7882],\n",
      "         [ 0.0824,  0.7020,  0.7333,  ..., -0.7882, -0.7882, -0.7882],\n",
      "         [-0.0745,  0.4510,  0.4667,  ..., -0.8196, -0.8196, -0.8196]],\n",
      "\n",
      "        [[-0.8745, -0.7961, -0.7961,  ...,  0.6784,  0.6235,  0.5451],\n",
      "         [-0.8667, -0.7882, -0.7882,  ...,  0.7569,  0.7020,  0.6314],\n",
      "         [-0.8667, -0.7882, -0.7882,  ...,  0.7569,  0.7020,  0.6392],\n",
      "         ...,\n",
      "         [ 0.0824,  0.7176,  0.7333,  ..., -0.7882, -0.7882, -0.7882],\n",
      "         [ 0.0824,  0.7020,  0.7333,  ..., -0.7882, -0.7882, -0.7882],\n",
      "         [-0.0745,  0.4510,  0.4667,  ..., -0.8196, -0.8196, -0.8196]],\n",
      "\n",
      "        [[-0.8745, -0.7961, -0.7961,  ...,  0.6784,  0.6235,  0.5451],\n",
      "         [-0.8667, -0.7882, -0.7882,  ...,  0.7569,  0.7020,  0.6314],\n",
      "         [-0.8667, -0.7882, -0.7882,  ...,  0.7569,  0.7020,  0.6392],\n",
      "         ...,\n",
      "         [ 0.0824,  0.7176,  0.7333,  ..., -0.7882, -0.7882, -0.7882],\n",
      "         [ 0.0824,  0.7020,  0.7333,  ..., -0.7882, -0.7882, -0.7882],\n",
      "         [-0.0745,  0.4510,  0.4667,  ..., -0.8196, -0.8196, -0.8196]]]), tensor(0), tensor(0)]\n"
     ]
    }
   ],
   "source": [
    "class LimitedDataset:\n",
    "    def __init__(self, full_dataset, limit=20000):\n",
    "        self.full_dataset = full_dataset\n",
    "        self.limit = min(limit, len(full_dataset))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index < self.limit:\n",
    "            return self.full_dataset[index]\n",
    "        else:\n",
    "            raise IndexError(\"Index out of range\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.limit\n",
    "\n",
    "# Example usage:\n",
    "dataset = LimitedDataset(dataset)\n",
    "\n",
    "# Now you can use limited_dataset with indexing up to 20,000\n",
    "print(len(dataset))  # Should print 20000\n",
    "print(dataset[19999])  # Accessing the last item within the limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, val_test_data = train_test_split(dataset, test_size=0.2)\n",
    "# val_data, test_data = train_test_split(val_test_data, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data and create dataloaders\n",
    "train_dataloader = DataLoader(dataset, batch_size=args.train_batch_size, shuffle=False)\n",
    "# val_dataloader = DataLoader(val_data, batch_size=args.valid_batch_size, shuffle=False)\n",
    "# test_dataloader = DataLoader(test_data, batch_size=args.test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=args.num_labels)\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\",\n",
    "                                                  config=config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 417/417 [33:49<00:00,  4.87s/batch]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 37\u001b[0m\n\u001b[0;32m     28\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#     for images, _, labels in val_dataloader:\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#         images, labels = images.to(device), labels.to(device)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#         total += labels.size(0)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#         correct += (predicted == labels).sum().item()\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m100\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcorrect\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\n\u001b[0;32m     38\u001b[0m val_accuracies\u001b[38;5;241m.\u001b[39mappend(val_accuracy)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, _, labels in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{args.num_epochs}\", unit=\"batch\", total=len(train_dataloader)):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # with torch.no_grad():\n",
    "    #     for images, _, labels in val_dataloader:\n",
    "    #         images, labels = images.to(device), labels.to(device)\n",
    "    #         outputs = model(images).logits\n",
    "    #         _, predicted = outputs.max(1)\n",
    "    #         total += labels.size(0)\n",
    "    #         correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = 100 * correct / total\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{args.num_epochs}, Loss: {avg_train_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0053239938952654"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_loss / len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    for images, text, labels in test_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).logits\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "light",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
